# ğŸ”® Next Word Prediction using LSTM & GPT-2 Transformers (Fine-Tuned on WikiText-2)

This project explores and compares two deep learning approaches for **Next Word Prediction**:

- ğŸ“š **LSTM-based model** (built from scratch using Keras)
- ğŸ¤– **Transformer-based model** (fine-tuned GPT-2 using Hugging Face Transformers)

Both models are trained on the [WikiText-2](https://huggingface.co/datasets/wikitext) dataset and evaluated on **loss** and **perplexity** to determine performance.

---

## ğŸš€ Project Structure

```
ğŸ“ next_word_prediction_project/
â”‚
â”œâ”€â”€ lstm_model/
â”‚   â”œâ”€â”€ final_lstm_model.h5
â”‚   â”œâ”€â”€ model.weights.h5
â”‚   â””â”€â”€ tokenizer.pkl
â”‚
â”œâ”€â”€ transformers_model/
â”‚   â””â”€â”€ gpt2-wikitext-best-model/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â””â”€â”€ tokenizer.json
â”‚
â”œâ”€â”€ comparison_plot.png
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
```

---

## ğŸ§  Models Used

### âœ… LSTM (Keras)
- Tokenized using `Tokenizer`
- Created n-gram sequences of size 4
- Trained with a Bidirectional LSTM layer
- Metrics: `accuracy`, `top_5_accuracy`
- Saved: `.h5` model, tokenizer, and weights

### âœ… GPT-2 Transformer (Hugging Face)
- Fine-tuned using `Trainer` on grouped token blocks
- Used `GPT2Config` with dropout adjustment
- Early stopping, cosine learning rate scheduling
- Evaluation Metric: `eval_loss` and Perplexity
- Saved best model and tokenizer

---

## ğŸ“Š Model Comparison

| Metric       | LSTM Model | GPT-2 Transformer |
|--------------|------------|-------------------|
| **Loss**     | 4.5        | 3.065             |
| **Perplexity** | ~90       | **21.44**         |

> GPT-2 outperforms LSTM on loss and perplexity.

---

## âš™ï¸ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/next-word-prediction.git
cd next-word-prediction
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ“‚ Dataset

Dataset used: **WikiText-2**

```python
from datasets import load_dataset
dataset = load_dataset("wikitext", "wikitext-2-raw-v1")
```

---

## ğŸ‹ï¸â€â™‚ï¸ Training

### â¤ LSTM Training

```bash
python train_lstm_model.py
```

Saves to:
- `final_lstm_model.h5`
- `tokenizer.pkl`
- `model.weights.h5`

### â¤ GPT-2 Fine-Tuning

```bash
python train_transformer.py
```

Outputs:
- `transformers_model/gpt2-wikitext-best-model/`

---

## ğŸ§ª Inference

### â¤ Using Fine-Tuned GPT-2

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained("transformers_model/gpt2-wikitext-best-model")
tokenizer = AutoTokenizer.from_pretrained("transformers_model/gpt2-wikitext-best-model")

prompt = "The history of India begins with"
inputs = tokenizer(prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0]))
```

---

## ğŸ“ˆ Results Visualization

![Comparison](comparison_plot.png)

---

## ğŸ“Œ Key Learnings

- LSTM is simple and requires manual preprocessing.
- Transformers handle longer context and outperform in prediction accuracy.
- Hugging Face tools simplify training and evaluation pipelines.

---

## ğŸ§  Authors & Credits

- **Developer:** R R Shanmugapriya
- **Dataset:** [WikiText-2](https://huggingface.co/datasets/wikitext)
- **Frameworks:** TensorFlow, PyTorch, Hugging Face Transformers

---

## ğŸ“„ License

This project is licensed under the MIT License.
